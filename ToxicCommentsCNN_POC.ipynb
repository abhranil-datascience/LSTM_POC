{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ToxicCommentsCNN_POC.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhranil-datascience/LSTM_POC/blob/master/ToxicCommentsCNN_POC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOUucuQKiUa4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############################## Mount Drive ######################################## \n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "############################## Change Directory ###################################\n",
        "#import os\n",
        "#os.chdir('/content/gdrive/My Drive/MLandDLFullCourse/DL/AdvancedNLP/1.ToxicComments/Downloads')\n",
        "#!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "#!ls\n",
        "#!unzip toxic-comments-classification-master.zip glove.6B.zip\n",
        "#!unzip glove.6B.zip\n",
        "######## Change to Appropriate Directory ############\n",
        "import os\n",
        "os.chdir('/content/gdrive/My Drive/MLandDLFullCourse/DL/AdvancedNLP/1.ToxicComments')\n",
        "################## Declaring Variables #########################\n",
        "MaxVocabSize=20000\n",
        "Dimension=100\n",
        "BestNLPModel=\"BestToxicCommentsCNN.hdf5\"\n",
        "BatchSize=150\n",
        "MaxSequenceLengthTrain=0\n",
        "################## Step 1: Read Dataset ########################\n",
        "\n",
        "import pandas as pd\n",
        "### Training ###\n",
        "TrainDataset=pd.read_csv('train.csv')\n",
        "XTrain=TrainDataset.iloc[:,1:2].fillna(\"DUMMY_VALUE\").values\n",
        "YTrain=TrainDataset.iloc[:,2:8].values\n",
        "### Testing ###\n",
        "TestDataset=pd.read_csv('test.csv')\n",
        "XTest=TestDataset.iloc[:,1:2].fillna(\"DUMMY_VALUE\").values\n",
        "\n",
        "########## Step 2: Load pretrained wordvectors #################\n",
        "\n",
        "def LoadPretrainedVectors():\n",
        "  path_to_embedding_matrix=\"/content/gdrive/My Drive/MLandDLFullCourse/DL/AdvancedNLP/1.ToxicComments/Downloads/Glove6BUnzipped/glove.6B.\"+str(Dimension)+\"d.txt\"\n",
        "  import numpy as np\n",
        "  WordToVec={}\n",
        "  with open(path_to_embedding_matrix) as f:\n",
        "    for line in f:\n",
        "      values=line.split()\n",
        "      word=values[0]\n",
        "      vec=np.array(values[1:],dtype=float)\n",
        "      WordToVec[word]=vec\n",
        "  return WordToVec\n",
        "    \n",
        "############### Step 3: Tokenize Each Sentence ###################\n",
        "\n",
        "def TokenizeData(Dataset):\n",
        "  from keras.preprocessing.text import Tokenizer\n",
        "  tokenizer=Tokenizer(num_words=MaxVocabSize)\n",
        "  tokenizer.fit_on_texts(Dataset[:,0])\n",
        "  datasequences=tokenizer.texts_to_sequences(Dataset[:,0])\n",
        "  ### Check if first item in the sequences correctly represents all words ###\n",
        "  #for item in datasequences[0]:\n",
        "  #  for word,index in tokenizer.word_index.items():\n",
        "  #    if(index==item):\n",
        "  #      print(\"Word is: \"+word+\" || Index is \"+str(index))\n",
        "  ### Check min and max sequence length ###\n",
        "  #print (\"Min sequence length: \"+ str(min(len(s) for s in datasequences)))\n",
        "  #print (\"Max sequence length: \"+ str(max(len(s) for s in datasequences)))\n",
        "  return tokenizer,datasequences\n",
        "\n",
        "########### Step 4: Create Word To Integer Mapping ##################\n",
        "\n",
        "def CreateIntToWordMapping(DataTokenizer):\n",
        "  IntToWordMapping={val:word for word,val in DataTokenizer.word_index.items() if val<=MaxVocabSize}\n",
        "  return IntToWordMapping\n",
        "\n",
        "#################### Step 5: Pad Sequences ##########################\n",
        "\n",
        "def PadData(InputSequence,MaxSequenceLength):\n",
        "  from keras.preprocessing.sequence import pad_sequences\n",
        "  PaddedSequence=pad_sequences(sequences=InputSequence,padding='post',maxlen=MaxSequenceLength)\n",
        "  return PaddedSequence\n",
        "\n",
        "############### Step 6: Create Embedding Matrix #####################\n",
        "\n",
        "def CreateEmbeddingMatrix(Mapping):\n",
        "  Max_Num_Of_Words=min(MaxVocabSize,len(Mapping))\n",
        "  import numpy as np\n",
        "  Embedded=np.zeros((Max_Num_Of_Words,Dimension))\n",
        "  for items in Mapping:\n",
        "    EmbeddedValue=None\n",
        "    CurrentWord=Mapping[items]\n",
        "    try:\n",
        "      EmbeddedValue=WordToVec[CurrentWord]\n",
        "    except:\n",
        "      ()\n",
        "    if(EmbeddedValue is not None):\n",
        "      Embedded[items-1]=EmbeddedValue\n",
        "  return Embedded, Max_Num_Of_Words\n",
        "    \n",
        "############### Step 7: Create Embedding Layer #######################\n",
        "\n",
        "def CreateModel(num_of_words,EmbeddedMatrix,PaddedDataset,OutputLabel):\n",
        "  from keras.layers import Embedding\n",
        "  EmbeddedLayer=Embedding(input_dim=num_of_words,output_dim=Dimension,weights=[EmbeddedMatrix],input_length=PaddedDataset.shape[1],trainable=False)\n",
        "  ################## Step 8: Create Model #######################\n",
        "  from keras.layers import Dense, Input, Conv1D, MaxPool1D, GlobalMaxPool1D, Dropout,BatchNormalization, Flatten\n",
        "  from keras.models import Model\n",
        "  InputData=Input(shape=(PaddedDataset.shape[1],))\n",
        "  x=EmbeddedLayer(InputData)\n",
        "  ###################### Layer 1 #############################\n",
        "  x=Conv1D(filters=128,kernel_size=3,strides=1,activation='relu')(x)\n",
        "  x=MaxPool1D(pool_size=3,strides=1)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dropout(0.2)(x)\n",
        "  ###################### Layer 2 #############################\n",
        "  x=Conv1D(filters=256,kernel_size=3,strides=1,activation='relu')(x)\n",
        "  x=MaxPool1D(pool_size=3,strides=1)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dropout(0.2)(x)\n",
        "  ###################### Layer 3 #############################\n",
        "  x=Conv1D(filters=512,kernel_size=3,strides=1,activation='relu')(x)\n",
        "  x=MaxPool1D(pool_size=3,strides=1)(x)\n",
        "  x=Flatten()(x)\n",
        "  ###################### Layer 4 #############################\n",
        "  x=Dense(units=256,activation='relu',kernel_initializer='random_uniform')(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dropout(0.2)(x)\n",
        "  ###################### Layer 5 #############################\n",
        "  x=Dense(units=128,activation='relu',kernel_initializer='random_uniform')(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dropout(0.2)(x)\n",
        "  ###################### Layer 6 #############################\n",
        "  x=Dense(units=64,activation='relu',kernel_initializer='random_uniform')(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dropout(0.2)(x)\n",
        "  ###################### Layer 7 #############################\n",
        "  output=Dense(units=OutputLabel.shape[1],activation='sigmoid',kernel_initializer='random_uniform')(x)\n",
        "  #################### Compile Model ###############################\n",
        "  model_CNN=Model(InputData,output)\n",
        "  model_CNN.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "  return model_CNN\n",
        "\n",
        "################## Step 9: Train Model ######################\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "#------------------>>>>>1\n",
        "WordToVec=LoadPretrainedVectors()\n",
        "#------------------>>>>>2\n",
        "TrainTokenizer,TrainDataSequence=TokenizeData(XTrain)\n",
        "#------------------>>>>>3\n",
        "TrainIntToWordMapping=CreateIntToWordMapping(TrainTokenizer)\n",
        "#------------------>>>>>4\n",
        "MaxSequenceLengthTrain=max(len(s) for s in TrainDataSequence)\n",
        "PaddedTrainSequence=PadData(TrainDataSequence,MaxSequenceLengthTrain)\n",
        "#------------------>>>>>5\n",
        "EmbeddedMatrixTrain, MaxNumOfWordsTrain=CreateEmbeddingMatrix(TrainIntToWordMapping)\n",
        "#------------------>>>>>6\n",
        "CNNModel=CreateModel(MaxNumOfWordsTrain,EmbeddedMatrixTrain,PaddedTrainSequence,YTrain)\n",
        "#------------------>>>>>7\n",
        "SaveBestModel=ModelCheckpoint(filepath=BestNLPModel,monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "CNNModel.fit(x=PaddedTrainSequence,y=YTrain,epochs=10,batch_size=BatchSize,validation_split=0.2,callbacks=[SaveBestModel])\n",
        "##### Validation Accuracy: 0.97751\n",
        "################ Step 10: Predict #######################\n",
        "from keras.models import load_model\n",
        "TestModel=load_model(BestNLPModel)\n",
        "TestTokenizer,TestDataSequence=TokenizeData(XTest)\n",
        "PaddedTestSequence=PadData(TestDataSequence,MaxSequenceLengthTrain)\n",
        "Prediction=TestModel.predict(PaddedTestSequence)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}